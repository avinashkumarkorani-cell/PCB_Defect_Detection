{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "R51EJyR5xDpr",
        "outputId": "3c2b1212-67bb-485b-d535-04a97255530e"
      },
      "outputs": [],
      "source": [
        "# Install and authenticate Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload your kaggle.json (API token file)\n",
        "from google.colab import files\n",
        "files.upload()   # Upload kaggle.json from your Kaggle account\n",
        "\n",
        "# Make a directory for Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj8vQkBYxGAr",
        "outputId": "00488168-924a-4760-9dfc-fde1598dc6f7"
      },
      "outputs": [],
      "source": [
        "# Download the dataset from Kaggle\n",
        "!kaggle datasets download -d akhatova/pcb-defects\n",
        "\n",
        "# Unzip dataset\n",
        "!unzip -q pcb-defects.zip -d ./pcb_defects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tj0Da7Ow33Y"
      },
      "outputs": [],
      "source": [
        "#Importing the standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('pastel')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2UqsWVjw33a"
      },
      "source": [
        "# **Understanding the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7lX3h6Hw33b",
        "outputId": "d28c4b4c-4700-4466-8eed-2bc38ec4b101"
      },
      "outputs": [],
      "source": [
        "#Defining the input\n",
        "input_dir= \"pcb_defects/PCB_DATASET\"\n",
        "os.listdir(input_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8X79UNKw33c"
      },
      "source": [
        "*  **PCB USED folder**- contains the 12 template images we used in the dataset.\n",
        "* **images folder**- contains the PCB images subclassed into different types.\n",
        "* **rotation folder**- contains the rotated PCB images subclassed into different types as well as rotation angle\n",
        "* **annotations folder**- contains the annotations for bounding box of each images\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJyn3HnZw33c"
      },
      "source": [
        "**Analyzing the PCB USED folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_4UmHVTVw33c",
        "outputId": "15277f01-ddc6-42d2-8b13-12d1de1f816f"
      },
      "outputs": [],
      "source": [
        "template_dir=os.path.join(input_dir,'PCB_USED')\n",
        "template_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-a0uz8Tw33c"
      },
      "outputs": [],
      "source": [
        "#Creating function to visualize images:\n",
        "def visualize_img(dir_name,nos_):\n",
        "    k=1\n",
        "    plt.figure(figsize=(8,(nos_//2)*6))\n",
        "    for filename in os.listdir(dir_name)[0:nos_]:\n",
        "        if filename.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "            ax=plt.subplot((nos_//2)+1,2,k)\n",
        "            img_path=os.path.join(dir_name,filename)\n",
        "            img=plt.imread(img_path)\n",
        "            ax.imshow(img)\n",
        "            ax.set_xlabel(filename)\n",
        "            ax.grid(False)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            k+=1\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "uyaYD3h2w33c",
        "outputId": "790e6ac7-ca20-4fc3-a3a9-843ba976b1ac"
      },
      "outputs": [],
      "source": [
        "visualize_img(template_dir,nos_=4)\n",
        "\n",
        "print(f'No of template images:{len(os.listdir(template_dir))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1aE6EPsw33d"
      },
      "source": [
        "**Analyzing the images folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwoPNthew33d",
        "outputId": "e0e20915-ea82-4562-fae0-4881b7f4d79b"
      },
      "outputs": [],
      "source": [
        "#Defining the image directory\n",
        "img_dir=os.path.join(input_dir,'images')\n",
        "\n",
        "#Listing the types of defects\n",
        "os.listdir(img_dir)\n",
        "types_defect=os.listdir(os.path.join(input_dir,'images'))\n",
        "types_defect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRK5kMd5w33d"
      },
      "outputs": [],
      "source": [
        "#Creating an image path list for ready refernce\n",
        "img_path_list=[]\n",
        "#Creating img_path list\n",
        "for sub_cat in types_defect:\n",
        "    for file in os.listdir(os.path.join(img_dir,sub_cat)):\n",
        "\n",
        "        img_path_list.append(os.path.join(img_dir,sub_cat,file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wk-rgnfEw33d",
        "outputId": "7997957e-5871-4732-d51c-04fe10c53913"
      },
      "outputs": [],
      "source": [
        "#Vizualizing defect images for each type\n",
        "df_defect=pd.DataFrame(columns=['No of defect']) #dataframe for counting no of defects\n",
        "for sub_cat in types_defect:\n",
        "    visualize_img(os.path.join(img_dir,sub_cat),nos_=2)  #Visualizing 2 types of defect for each type\n",
        "\n",
        "    print(f'No of {sub_cat} images:{len(os.listdir(os.path.join(img_dir,sub_cat)))}')\n",
        "\n",
        "    df_defect.loc[sub_cat]=len(os.listdir(os.path.join(img_dir,sub_cat)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "EOEtuB8Ew33d",
        "outputId": "4b56508c-ee76-4021-8f4f-43026570500d"
      },
      "outputs": [],
      "source": [
        "#No. of defects by type\n",
        "df_defect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r1h13a9w33d"
      },
      "source": [
        "**Analyzing rotated folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqXK1tHUw33e",
        "outputId": "cce8ff5c-9f62-42e5-f797-aef1960a21f3"
      },
      "outputs": [],
      "source": [
        "rotated_dir=os.path.join(input_dir,'rotation')\n",
        "os.listdir(rotated_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC-QfOo6w33e",
        "outputId": "c0cdf7f5-4316-464e-b769-deb406002f9a"
      },
      "outputs": [],
      "source": [
        "rotated_angle_list=[j for j in os.listdir(rotated_dir) if j.endswith('.txt')]\n",
        "rotated_angle_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CdjwVViw33e",
        "outputId": "5fd25149-e1c2-4437-f959-b8e974393f1c"
      },
      "outputs": [],
      "source": [
        "types_defect_rotated=[j for j in os.listdir(rotated_dir) if j.endswith('.txt')==False]\n",
        "types_defect_rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CFC3yt38w33e",
        "outputId": "bda22fb1-a205-4104-8281-838331ada74b"
      },
      "outputs": [],
      "source": [
        "#Vizualizing rotated defects\n",
        "df_defect_rotated=pd.DataFrame(columns=['No of defect'])\n",
        "for sub_cat in types_defect_rotated:\n",
        "    visualize_img(os.path.join(rotated_dir,sub_cat),nos_=2)\n",
        "\n",
        "    print(f'No of {sub_cat} images:{len(os.listdir(os.path.join(rotated_dir,sub_cat)))}')\n",
        "\n",
        "    df_defect_rotated.loc[sub_cat]=len(os.listdir(os.path.join(rotated_dir,sub_cat)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "RMORUVyiw33e",
        "outputId": "8eb81196-a8cd-465f-d960-209f39841e13"
      },
      "outputs": [],
      "source": [
        "#No. of defects by type  rotated\n",
        "df_defect_rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibKt_f96w33e"
      },
      "outputs": [],
      "source": [
        "#Reading the rotation text files\n",
        "\n",
        "df_rotation_angle=pd.DataFrame(columns=['Line','Angle'])\n",
        "for filename in rotated_angle_list:\n",
        "    with open(os.path.join(rotated_dir,filename),'r') as f:\n",
        "        lines=f.readlines()\n",
        "        for line in lines:\n",
        "            text,angle=line.split()\n",
        "            df_rotation_angle=pd.concat([df_rotation_angle,pd.DataFrame({'Line':[text],'Angle':[angle]})],axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "XdoGHkuGw33e",
        "outputId": "fdeb45f0-032a-4755-f87e-52bd1f70582b"
      },
      "outputs": [],
      "source": [
        "df_rotation_angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN0_cFmDw33f"
      },
      "source": [
        "**Analyzing the annotate folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "od0d9W7Jw33f",
        "outputId": "2aa86630-1557-4c86-ce4b-6c1b1f9707ba"
      },
      "outputs": [],
      "source": [
        "annote_dir=os.path.join(input_dir,'Annotations')\n",
        "annote_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5LA_spiw33f",
        "outputId": "9fae8fe1-3201-4d17-9746-8725cc43d758"
      },
      "outputs": [],
      "source": [
        "type_annot=os.listdir(annote_dir)\n",
        "type_annot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "YBj-Ymodw33f",
        "outputId": "adf9a6b3-cefd-4120-8118-baa19c98c859"
      },
      "outputs": [],
      "source": [
        "df_annot_nos=pd.DataFrame(columns=['No of annotations'])\n",
        "#Checking the length of annotation itms\n",
        "for i in type_annot:\n",
        "    df_annot_nos.loc[i]=len(os.listdir(os.path.join(annote_dir,i)))\n",
        "df_annot_nos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR7A6RHjw33f"
      },
      "source": [
        "We see that we have an annotation file for each image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SOXSuCYw33f",
        "outputId": "a810fc7c-9751-4f48-ab25-2b738e33f9d9"
      },
      "outputs": [],
      "source": [
        "#Checking the type of files\n",
        "file_list=os.listdir(os.path.join(annote_dir,'Mouse_bite'))\n",
        "file_list[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ZPTas_w33f"
      },
      "source": [
        "We see that all files are in XML format, so we have to parse the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INNHYZ5Gw33f"
      },
      "outputs": [],
      "source": [
        "#importing xml ET to parse xml file\n",
        "import xml.etree.ElementTree as ET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT8g0Fdcw33f"
      },
      "outputs": [],
      "source": [
        "tree = ET.parse(os.path.join(os.path.join(annote_dir,'Mouse_bite'),'01_mouse_bite_11.xml'))\n",
        "root = tree.getroot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMZV1UCCw33f",
        "outputId": "d213f4f7-8996-41b8-994c-0ba5ab77cedd"
      },
      "outputs": [],
      "source": [
        "#getting the structure of XML file\n",
        "print(ET.tostring(root, encoding='utf8').decode('utf8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOxstO9ow33f"
      },
      "outputs": [],
      "source": [
        "#Parsing XML to return Bounding box dimensions\n",
        "def parse_xml(xml_file):\n",
        "\n",
        "    data=[]\n",
        "\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    filename = root.find('filename').text\n",
        "    width = int(root.find('size/width').text)\n",
        "    height = int(root.find('size/height').text)\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        xmin = int(obj.find('bndbox/xmin').text)\n",
        "        ymin = int(obj.find('bndbox/ymin').text)\n",
        "        xmax = int(obj.find('bndbox/xmax').text)\n",
        "        ymax = int(obj.find('bndbox/ymax').text)\n",
        "\n",
        "        data.append({\n",
        "            'filename': filename,\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'class': name,\n",
        "            'xmin': xmin,\n",
        "            'ymin': ymin,\n",
        "            'xmax': xmax,\n",
        "            'ymax': ymax\n",
        "        })\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkCzi-ONw33g"
      },
      "outputs": [],
      "source": [
        "#Retrieving data for all files\n",
        "data=[]\n",
        "all_data=[]\n",
        "\n",
        "for x in type_annot:\n",
        "    for file in os.listdir(os.path.join(annote_dir,x)):\n",
        "        xml_file_path=os.path.join(os.path.join(annote_dir,x),file)\n",
        "        data=parse_xml(xml_file_path)\n",
        "        all_data.extend(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2cv_ip7Mw33g",
        "outputId": "08a695c5-7d32-4575-c359-16495773c307"
      },
      "outputs": [],
      "source": [
        "#Creating a dataframe to store the annotations\n",
        "df_annot=pd.DataFrame(all_data)\n",
        "df_annot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXeufWK_w33g",
        "outputId": "fc40f377-c235-4193-b04e-0e473239cafe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Path to the main images folder in Colab (already uploaded)\n",
        "img_dir = \"/content/pcb_defects/PCB_DATASET/images\"  # <-- Correct path\n",
        "\n",
        "# Define the source directories for each category inside img_dir\n",
        "source_dirs = [\n",
        "    os.path.join(img_dir, \"Missing_hole\"),\n",
        "    os.path.join(img_dir, \"Mouse_bite\"),\n",
        "    os.path.join(img_dir, \"Open_circuit\"),\n",
        "    os.path.join(img_dir, \"Short\"),\n",
        "    os.path.join(img_dir, \"Spur\"),\n",
        "    os.path.join(img_dir, \"Spurious_copper\")\n",
        "]\n",
        "\n",
        "# Define the destination directory for the combined images (Writable location in Colab)\n",
        "destination_dir = \"/content/images_combined\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each source directory and copy all files to the destination\n",
        "for source_dir in source_dirs:\n",
        "    if os.path.exists(source_dir):\n",
        "        # Get all files in the current directory\n",
        "        files = os.listdir(source_dir)\n",
        "\n",
        "        # Copy each file to the destination directory\n",
        "        for file in files:\n",
        "            file_path = os.path.join(source_dir, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                shutil.copy(file_path, destination_dir)\n",
        "    else:\n",
        "        print(f\"Directory {source_dir} does not exist.\")\n",
        "\n",
        "# Now check how many files are in the destination folder\n",
        "files_in_combined = os.listdir(destination_dir)\n",
        "print(f\"Number of files copied: {len(files_in_combined)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TmZL2ZUFw33g",
        "outputId": "48450c5a-7228-4d16-d2b4-be585d51bd54"
      },
      "outputs": [],
      "source": [
        "destination_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "QqWKHi3uw33g",
        "outputId": "523afe7b-9b49-41eb-ec0a-95c304cb5cc4"
      },
      "outputs": [],
      "source": [
        "#Visualizing the no of defects in each pcb\n",
        "df_multiple_defects=pd.DataFrame(df_annot['filename'].value_counts())\n",
        "sns.countplot(df_multiple_defects,x='count')\n",
        "plt.xlabel('No of defects in one PCB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKFvLeTQw33h"
      },
      "outputs": [],
      "source": [
        "#Defining a function to view image along with bounding box\n",
        "\n",
        "def draw_bounding_boxes(image_path, bounding_boxes,annotation):\n",
        "    \"\"\"\n",
        "    Draws multiple bounding boxes on an image using Matplotlib.\n",
        "\n",
        "    Args:\n",
        "        image_path: The path to the image file.\n",
        "        bounding_boxes: A list of bounding boxes, each represented as a tuple or list containing\n",
        "                       (min_x, min_y, max_x, max_y).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the image\n",
        "    img = plt.imread(image_path)\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(15,10))\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Draw each bounding box\n",
        "    for bbox in bounding_boxes:\n",
        "        min_x, min_y, max_x, max_y = bbox\n",
        "        width = max_x - min_x\n",
        "        height = max_y - min_y\n",
        "        rect = patches.Rectangle((min_x, min_y), width, height, linewidth=1, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Calculate the centroid of the bounding box\n",
        "        centroid_x = (min_x + max_x) / 2\n",
        "        centroid_y = (min_y + max_y) / 2\n",
        "\n",
        "        # Add the annotation to the centroid\n",
        "        ax.annotate( annotation,(centroid_x,centroid_y),(max_x+20,max_y+20),\n",
        "            fontsize=10,color='white',\n",
        "            horizontalalignment='right', verticalalignment='top')\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D0TMj-TLw33h",
        "outputId": "eb180bfc-3f53-4ac6-c031-620dafdc4a41"
      },
      "outputs": [],
      "source": [
        "#Getting filename from filepath\n",
        "filepath=img_path_list[0]\n",
        "filename=re.sub(r'.+/([\\w_]+\\.jpg)',r'\\1',filepath)\n",
        "filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2uOdIaVw33i"
      },
      "outputs": [],
      "source": [
        "def visualize_annotations(list_image_path, df):\n",
        "    for i in list_image_path:\n",
        "        filepath = i\n",
        "        filename = re.sub(r'.+/([\\w_]+\\.jpg)', r'\\1', filepath)\n",
        "        df_selected = df[df['filename'] == filename]\n",
        "        width  = df_selected['width'].values\n",
        "        height = df_selected['height'].values\n",
        "\n",
        "        # check if 'class' or 'class_name' exists\n",
        "        if 'class_name' in df_selected.columns:\n",
        "            class_name = df_selected['class_name'].values\n",
        "        elif 'class' in df_selected.columns:\n",
        "            class_name = df_selected['class'].values\n",
        "        else:\n",
        "            raise KeyError(\"No class column found in dataframe\")\n",
        "\n",
        "        xmin = df_selected['xmin'].values\n",
        "        ymin = df_selected['ymin'].values\n",
        "        xmax = df_selected['xmax'].values\n",
        "        ymax = df_selected['ymax'].values\n",
        "\n",
        "        bbox = zip(xmin, ymin, xmax, ymax)\n",
        "        draw_bounding_boxes(filepath, bbox, class_name[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aM3bDPedw33i",
        "outputId": "ae372192-27ca-419c-ab0c-1b499d7b8537"
      },
      "outputs": [],
      "source": [
        "image_path_shuffle=img_path_list\n",
        "random.shuffle(image_path_shuffle)\n",
        "\n",
        "visualize_annotations(image_path_shuffle[0:5],df_annot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JTtBXj6uw33i",
        "outputId": "f7be6d90-c731-4fe2-93a6-acec1abdde51"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 1. IMPORTING LIBRARIES AND CONFIGURATION\n",
        "# ===================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms, utils\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# Basic configuration\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. DATA PREPARATION (using df_annot and img_dir)\n",
        "# ===================================================================\n",
        "# ASSUMING THAT 'df_annot' (DataFrame) AND 'img_dir' (str) ALREADY EXIST.\n",
        "\n",
        "img_dir = destination_dir\n",
        "\n",
        "# To avoid conflicts with the Python keyword 'class', we rename the column\n",
        "if 'class' in df_annot.columns:\n",
        "    df_annot = df_annot.rename(columns={'class': 'class_name'})\n",
        "\n",
        "print(\"Preview of the provided annotation DataFrame:\")\n",
        "print(df_annot.head())\n",
        "\n",
        "# Get the list of classes and number of classes\n",
        "class_names = sorted(df_annot['class_name'].unique())\n",
        "num_classes = len(class_names)\n",
        "print(f\"\\n{num_classes} classes detected in the DataFrame: {class_names}\")\n",
        "\n",
        "# --- Train/Validation/Test split based on FILE NAMES ---\n",
        "# This is CRUCIAL to avoid data leakage.\n",
        "unique_filenames = df_annot['filename'].unique()\n",
        "train_files, test_val_files = train_test_split(unique_filenames, test_size=0.3, random_state=42)\n",
        "val_files, test_files = train_test_split(test_val_files, test_size=0.5, random_state=42) # 0.3 * 0.5 = 0.15\n",
        "\n",
        "# Create DataFrames for each dataset\n",
        "train_df = df_annot[df_annot['filename'].isin(train_files)].reset_index(drop=True)\n",
        "val_df = df_annot[df_annot['filename'].isin(val_files)].reset_index(drop=True)\n",
        "test_df = df_annot[df_annot['filename'].isin(test_files)].reset_index(drop=True)\n",
        "\n",
        "dataset_sizes = {'train': len(train_df), 'val': len(val_df), 'test': len(test_df)}\n",
        "print(f\"Number of defects - Training: {dataset_sizes['train']}, Validation: {dataset_sizes['val']}, Test: {dataset_sizes['test']}\")\n",
        "\n",
        "# --- Data Augmentation and Normalization ---\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)), # Standardize the size of cropped patches\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# --- Custom Dataset Class for Cropping Images ---\n",
        "class PCBCropDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, class_names, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_filename = row['filename']\n",
        "        box = (row['xmin'], row['ymin'], row['xmax'], row['ymax'])\n",
        "        label_idx = self.class_to_idx[row['class_name']]\n",
        "\n",
        "        try:\n",
        "            img_path = os.path.join(self.image_dir, img_filename)\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            cropped_image = image.crop(box)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File not found {img_path}. Returning an empty tensor.\")\n",
        "            return torch.zeros((3, 224, 224)), -1 # Handle error case\n",
        "\n",
        "        if self.transform:\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        return cropped_image, label_idx\n",
        "\n",
        "# --- Creating Datasets and DataLoaders ---\n",
        "train_dataset = PCBCropDataset(train_df, img_dir, class_names, transform=data_transforms['train'])\n",
        "val_dataset = PCBCropDataset(val_df, img_dir, class_names, transform=data_transforms['val'])\n",
        "test_dataset = PCBCropDataset(test_df, img_dir, class_names, transform=data_transforms['val'])\n",
        "\n",
        "# Use a smaller batch size to start and avoid memory errors\n",
        "batch_size = 16\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count()),\n",
        "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count()),\n",
        "    'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count())\n",
        "}\n",
        "\n",
        "# --- Visualizing a batch of cropped defect patches ---\n",
        "print(\"\\nVisualizing a batch of cropped defect patches...\")\n",
        "inputs, classes_idx = next(iter(dataloaders['train']))\n",
        "out = utils.make_grid(inputs)\n",
        "\n",
        "# Reverse normalization for display\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean; inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=(15, 8)); plt.imshow(inp)\n",
        "    if title is not None: plt.title(title)\n",
        "    plt.axis('off'); plt.show()\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes_idx])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. PRETRAINED MODEL (ResNet18 - most performant approach)\n",
        "# ===================================================================\n",
        "def get_pretrained_model(num_classes):\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    # Freeze the weights of the pretrained layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the final layer to adapt it to our problem\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. TRAINING AND VALIDATION FUNCTION\n",
        "# ===================================================================\n",
        "def train_model(model, criterion, optimizer, num_epochs=25, patience=5):\n",
        "    since = time.time()\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}' + ' | ' + '-'*10)\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward(); optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "\n",
        "            if phase == 'val':\n",
        "                if epoch_loss < best_loss:\n",
        "                    print(f\"Validation loss improved ({best_loss:.4f} -> {epoch_loss:.4f}). Saving model...\")\n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    epochs_no_improve = 0\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
        "            break\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
        "    print(f'Best Validation Loss: {best_loss:4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. TRAINING EXECUTION\n",
        "# ===================================================================\n",
        "# Clear GPU cache to ensure memory is free\n",
        "if torch.cuda.is_available():\n",
        "    #torch.cuda.empty_cache()\n",
        "    pass\n",
        "\n",
        "# Instantiate the model\n",
        "resnet_model = get_pretrained_model(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Only optimize parameters of the new layer (those that are not frozen)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_model.parameters()), lr=0.001)\n",
        "\n",
        "# Start training\n",
        "best_resnet_model, history = train_model(resnet_model, criterion, optimizer, num_epochs=20, patience=5)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 6. ANALYSIS AND EVALUATION OF RESULTS\n",
        "# ===================================================================\n",
        "# --- Display learning curves ---\n",
        "def plot_history(history, model_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
        "    fig.suptitle(f\"Learning curves for {model_name}\", fontsize=16)\n",
        "    ax1.plot(history['train_acc'], label='Train Acc'); ax1.plot(history['val_acc'], label='Val Acc')\n",
        "    ax1.set_title('Accuracy'); ax1.set_xlabel('Epoch'); ax1.legend(); ax1.grid(True)\n",
        "    ax2.plot(history['train_loss'], label='Train Loss'); ax2.plot(history['val_loss'], label='Val Loss')\n",
        "    ax2.set_title('Loss'); ax2.set_xlabel('Epoch'); ax2.legend(); ax2.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history, \"ResNet18 on cropped patches\")\n",
        "\n",
        "# --- Evaluation on the test set ---\n",
        "def evaluate_model(model, dataloader, model_name):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs.to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n--- Final evaluation of model '{model_name}' on the Test Set ---\\n\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16)\n",
        "    plt.xlabel('Predictions'); plt.ylabel('True Labels')\n",
        "    plt.show()\n",
        "\n",
        "evaluate_model(best_resnet_model, dataloaders['test'], \"ResNet18\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UBIsmOfaMr8",
        "outputId": "c7999bd3-b05d-4489-9654-b395048435b5"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 7. SAVE THE TRAINED RESNET18 MODEL (SAFE WAY)\n",
        "# ===================================================================\n",
        "\n",
        "# Path to save inside Google Colab /content directory\n",
        "RESNET_MODEL_PATH = \"/content/resnet18_trained.pth\"\n",
        "\n",
        "# Save only the weights (state_dict) - safest method\n",
        "torch.save(best_resnet_model.state_dict(), RESNET_MODEL_PATH)\n",
        "\n",
        "print(f\"✅ Trained ResNet18 weights saved at: {RESNET_MODEL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qquEXBQBqNFW",
        "outputId": "06b66f33-3fc1-4b87-ae34-439c0b6c39a0"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 5. SAVE THE TRAINED CNN MODEL \"FROM SCRATCH\"\n",
        "# ===================================================================\n",
        "\n",
        "# Path to save inside Google Colab /content directory\n",
        "MODEL_PATH = \"/content/cnn_scratch_model.pth\"\n",
        "\n",
        "# Save the model's state_dict (recommended way in PyTorch)\n",
        "torch.save(best_cnn_scratch_model.state_dict(), MODEL_PATH)\n",
        "\n",
        "print(f\"✅ Model saved successfully at: {MODEL_PATH}\")\n",
        "\n",
        "# If you want to save the entire model (not just weights)\n",
        "MODEL_FULL_PATH = \"/content/cnn_scratch_model_full.pth\"\n",
        "torch.save(best_cnn_scratch_model, MODEL_FULL_PATH)\n",
        "\n",
        "print(f\"✅ Full model saved at: {MODEL_FULL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Pi29_pYw33p",
        "outputId": "e3ab48fe-3d30-4146-fbb7-e5dc2a897507"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Start of Fine-Tuning ResNet18 ---\")\n",
        "\n",
        "# Clear the GPU cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 1. Reload the pre-trained model\n",
        "finetune_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# 2. Unfreeze the last layers. For example, the last two blocks (layer3 and layer4)\n",
        "# Parameters are frozen by default, we selectively unfreeze them.\n",
        "for name, param in finetune_model.named_parameters():\n",
        "    if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# 3. Replace the classification head (as before)\n",
        "num_ftrs = finetune_model.fc.in_features\n",
        "finetune_model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "finetune_model = finetune_model.to(device)\n",
        "\n",
        "# 4. Create an optimizer with different learning rates (best practice)\n",
        "# A higher lr for the new layer, a very low lr for the unfrozen layers.\n",
        "optimizer_finetune = optim.Adam([\n",
        "    {'params': finetune_model.fc.parameters(), 'lr': 1e-3},\n",
        "    {'params': finetune_model.layer4.parameters(), 'lr': 1e-5}, # Very low lr\n",
        "    {'params': finetune_model.layer3.parameters(), 'lr': 1e-5}  # Very low lr\n",
        "])\n",
        "\n",
        "print(\"ResNet18 model prepared for fine-tuning.\")\n",
        "\n",
        "# 5. Launch training with the same function\n",
        "# The variables criterion, train_model, dataloaders, etc. already exist\n",
        "best_finetune_model, history_finetune = train_model(\n",
        "    finetune_model,\n",
        "    criterion,\n",
        "    optimizer_finetune,\n",
        "    num_epochs=5, # 20 epochs should be enough for fine-tuning\n",
        "    patience=5\n",
        ")\n",
        "\n",
        "# 6. Evaluate the fine-tuned model\n",
        "print(\"\\n--- Evaluation of the Fine-Tuned ResNet18 Model ---\")\n",
        "evaluate_model(best_finetune_model, dataloaders['test'], \"ResNet18 (Fine-Tuned)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEdUvDGQtbqJ",
        "outputId": "76c6f623-6d4c-4973-8b07-bfee60530339"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 7. SAVE THE FINE-TUNED RESNET18 MODEL\n",
        "# ===================================================================\n",
        "\n",
        "# Path to save inside Google Colab /content directory\n",
        "FINETUNE_MODEL_PATH = \"/content/resnet18_finetuned.pth\"\n",
        "\n",
        "# Save the model's state_dict (recommended)\n",
        "torch.save(best_finetune_model.state_dict(), FINETUNE_MODEL_PATH)\n",
        "print(f\"✅ Fine-tuned ResNet18 model weights saved at: {FINETUNE_MODEL_PATH}\")\n",
        "\n",
        "# Optionally, save the entire model (architecture + weights)\n",
        "FINETUNE_FULL_MODEL_PATH = \"/content/resnet18_finetuned_full.pth\"\n",
        "torch.save(best_finetune_model, FINETUNE_FULL_MODEL_PATH)\n",
        "print(f\"✅ Full fine-tuned ResNet18 model saved at: {FINETUNE_FULL_MODEL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YUK5KKtdCV",
        "outputId": "caed3509-a5df-4073-9233-82c2b82148e5"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 7. SAVE THE FINE-TUNED RESNET18 MODEL (RECOMMENDED WAY)\n",
        "# ===================================================================\n",
        "\n",
        "# Path to save inside Google Colab /content directory\n",
        "FINETUNE_MODEL_PATH = \"/content/resnet18_finetuned.pth\"\n",
        "\n",
        "# Save only the model weights (state_dict)\n",
        "torch.save(best_finetune_model.state_dict(), FINETUNE_MODEL_PATH)\n",
        "print(f\"✅ Fine-tuned ResNet18 model weights saved at: {FINETUNE_MODEL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "wpBf1xWTw33p",
        "outputId": "68a00e2c-b771-4ab1-d7a8-0dbc94a9db17"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 1. RETRIEVAL OF METRICS FOR THE 3 MODELS\n",
        "# ===================================================================\n",
        "# This function is a standalone version of the evaluation that returns the metrics.\n",
        "# It's cleaner than relying on the function from the previous cell.\n",
        "\n",
        "\n",
        "def get_final_metrics(model, dataloader, device, class_names):\n",
        "    \"\"\"Evaluate a model and return its accuracy and weighted F1-score.\"\"\"\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            # Handle potentially empty batches due to error filtering\n",
        "            if inputs.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            outputs = model(inputs.to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Generate the classification report as a dictionary\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
        "\n",
        "    accuracy = report['accuracy']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    return accuracy, f1_score\n",
        "\n",
        "print(\"Retrieving final metrics for each model on the test set...\")\n",
        "\n",
        "# Model 1: ResNet (Feature Extraction)\n",
        "resnet_acc, resnet_f1 = get_final_metrics(best_resnet_model, dataloaders['test'], device, class_names)\n",
        "\n",
        "# Model 2: CNN (From Scratch)\n",
        "cnn_acc, cnn_f1 = get_final_metrics(best_cnn_scratch_model, dataloaders['test'], device, class_names)\n",
        "\n",
        "# Model 3: ResNet (Fine-Tuned)\n",
        "finetune_acc, finetune_f1 = get_final_metrics(best_finetune_model, dataloaders['test'], device, class_names)\n",
        "\n",
        "print(\"\\n--- FINAL RESULTS ON THE TEST SET ---\")\n",
        "print(f\"ResNet (Feature Extract): Accuracy = {resnet_acc:.4f}, F1-Score = {resnet_f1:.4f}\")\n",
        "print(f\"CNN (From Scratch):       Accuracy = {cnn_acc:.4f}, F1-Score = {cnn_f1:.4f}\")\n",
        "print(f\"ResNet (Fine-Tuned):      Accuracy = {finetune_acc:.4f}, F1-Score = {finetune_f1:.4f}\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. COMPARATIVE VISUALIZATION WITH BAR CHARTS\n",
        "# ===================================================================\n",
        "\n",
        "model_labels = [\n",
        "    'ResNet\\n(Feature Extract)',\n",
        "    'CNN\\n(From Scratch)',\n",
        "    'ResNet\\n(Fine-Tuned)'\n",
        "]\n",
        "\n",
        "accuracies = [resnet_acc, cnn_acc, finetune_acc]\n",
        "f1_scores = [resnet_f1, cnn_f1, finetune_f1]\n",
        "\n",
        "x = np.arange(len(model_labels))  # Positions of labels on the x-axis\n",
        "width = 0.35  # Width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Create bars for accuracy and F1-score\n",
        "rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='cornflowerblue')\n",
        "rects2 = ax.bar(x + width/2, f1_scores, width, label='Weighted F1-Score', color='lightcoral')\n",
        "\n",
        "# Add text, titles, and labels\n",
        "ax.set_ylabel('Scores', fontsize=14)\n",
        "ax.set_title('Final Comparison of Model Performances', fontsize=18)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_labels, fontsize=12)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add value labels on each bar\n",
        "ax.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "ax.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "\n",
        "# Adjust y-axis limit to leave space for labels\n",
        "ax.set_ylim(0, max(max(accuracies), max(f1_scores)) * 1.15)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 1364422,
          "sourceId": 2266446,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
